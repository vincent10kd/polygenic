#' @title Pruning+thresholding polygenic risk score creation based on summary statistics
#'
#' @description Create a polygenic risk score based on summary statistics from prior GWAS/pQTL discovery studies.
#' Included variants in high LD can be decorrelated to prevent double-counting, using the conditional argument.
#'
#' @param variant_data An object of format output by extract_variants().
#' @param gwas_info An object generated by get_trait_variants() or get_pQTLs().
#' @param remove_indels If TRUE, removes indels.
#' @param imp_threshold Imputation quality threshold, based on R^2. Any variant with lower imputation R^2 is removed.
#' @param binary_outcome Set to TRUE for binary traits, and FALSE for continuous outcomes (including pQTLs).
#' @param exclude_extreme_associations If TRUE, removes variants with an odds ratio > 5 or <1/5.
#' @param LDplot If TRUE, plots the LD matrix (squared correlation matrix of variants).
#' @param pruning_threshold Variants in LD >= pruning_threshold with other variants are removed, keeping higher MAF variants.
#' @param pval_threshold Variants with GWAS p-values > pval_threshold are discarded. Set to 1 to turn off.
#' @param conditional If TRUE, uses marg2con() to create decorrelate variants in LD if these are within a given bp distance on the same chromosome.
#' @param cond_window A genomic distance within which to decorrelate variants in LD, in base pair.
#' @param cond_N The sample size of the original GWAS from which the marginal estimates were derived, or an approximation of it.
#' @param scale Centers and standardizes the polygenic risk score if TRUE.
#' @param flowchart If TRUE, plots a flowchart describing the creation of the polygenic risk score.
#'
#' @return A list containing several data.frames with all relevant information. The risk score is stored in element 'prs'.
#' @examples
#' # vte_prs <- create_prs(vte_extracted_variants, vte_gwas_info)
#' # hist(vte_prs$prs$prs)
#' @export
#' @importFrom PRISMAstatement "flow_exclusions"
#' @importFrom metafor "rma"

create_prs <- function (variant_data,
                        gwas_info,
                        remove_indels = FALSE,
                        imp_threshold = 0.8,
                        binary_outcome = TRUE,
                        exclude_extreme_associations = TRUE,
                        LDplot = FALSE,
                        pruning_threshold = 0.75,
                        pval_threshold = 5e-08,
                        conditional = FALSE,
                        cond_window = 10e6,
                        cond_N = 60000,
                        scale = FALSE,
                        flowchart = TRUE)
{
  start_time <- Sys.time()
  res_list <- list()
  e <- list()
  if (remove_indels == TRUE) {
    ndif <- nchar(variant_data$fix$REF) - nchar(variant_data$fix$ALT)
    if (any(ndif > 0)) {
      rind <- which(ndif > 0)
      variant_data$fix <- variant_data$fix[-rind, ]
      variants_left <- variant_data$fix$ID
      variant_data$gt <- variant_data$gt[, which(colnames(variant_data$gt) %in%
                                                   variants_left)]
    }
  }
  dups <- which(duplicated(data.frame(variant_data$fix)$ID))
  if (length(dups) > 0) {
    variant_data$fix <- variant_data$fix[-c(dups, dups - 1), ]
    variant_data$gt <- variant_data$gt[-c(dups, dups - 1), ]
    cat("> Removed", length(dups), "duplicate variants (e.g. triallelic SNPs).\n")
  }
  v <- variant_data$fix
  unique_variants <- unique(v$ID)
  no_of_variants <- length(unique_variants)
  cat("> A total of", no_of_variants, "variants was retrieved.\n")
  e$variants_retrieved <- no_of_variants
  drop_check <- function(v) {
    if (nrow(v) == 0)
      stop("All variants have been dropped. Try different tuning parameters.")
  }
  v$imp_qual <- v$DR2
  low_imp_qual_l <- length(unique(v$ID[v$imp_qual < imp_threshold]))
  v <- subset(v, imp_qual >= imp_threshold)
  cat("> Dropped", low_imp_qual_l, "variants with imputation R^2 below threshold.\n")
  e$low_imp_qual <- low_imp_qual_l
  v <- v[, -c(6:10)]
  unique_variants <- unique(v$ID)
  drop_check(v)
  varv <- suppressWarnings(sapply(variant_data$gt, sd))
  novar <- names(which(is.na(varv) | varv == 0))
  variant_data$gt <- variant_data$gt[, !colnames(variant_data$gt) %in% novar]
  v <- subset(v, !ID %in% novar)
  g <- gwas_info
  g <- subset(g, !variant_id %in% novar)
  l0 <- length(unique_variants) - length(unique(v$ID))
  unique_variants <- unique(v$ID)
  cat("> Dropped", l0, "variants with zero variance in the data.\n")
  e$no_variance <- l0
  drop_check(v)
  g <- subset(g, variant_id %in% unique_variants)
  l1 <- length(unique(v$ID[v$ID %in% unique(g$variant_id)]))
  cat("> Dropped", length(unique_variants) - l1, "variants that did not match with gwas_info.\n")
  e$missing_from_gwasinfo <- length(unique_variants) - l1
  g
  if (binary_outcome == TRUE) {
    g <- subset(g, !is.na(or_per_copy_number))
  }
  else {
    g <- subset(g, !is.na(beta_coefficient))
  }
  g <- g[!duplicated(g), ]
  l2 <- length(unique(v$ID[v$ID %in% unique(g$variant_id)]))
  cat("> Dropped", l1 - l2, "variants with missing effect sizes.\n")
  e$missing_effectsize <- l1 - l2
  if (binary_outcome == TRUE) {
    g$effect_size <- log(g$or_per_copy_number)
  }
  else {
    g$effect_size <- g$beta_coefficient
  }
  g <- subset(g, !is.na(risk_allele))
  l3 <- length(unique(v$ID[v$ID %in% unique(g$variant_id)]))
  cat("> Dropped", l2 - l3, "variants with missing risk alleles.\n")
  e$missing_risk_allele <- l2 - l3
  drop_check(v)

  # meta-analyze associations reported in >1 study
  snp_ind <- aggregate(effect_size ~ variant_id, g, function(x) length(unique(x)))
  multiOR_l <- sum(as.numeric(snp_ind[, 2] > 1))
  id_list <- list()
  for (i in 1:length(unique(snp_ind$variant_id))) {
    meta <- g[g$variant_id == unique(snp_ind$variant_id)[i], ]
    if (nrow(meta) > 1) {
      if (0 %in% meta$pvalue) {
        meta$pvalue[which(meta$pvalue == 0)] <- min(meta$pvalue[meta$pvalue > 0])
      }
      meta$standard_error <- ifelse(is.na(meta$standard_error),
                                    abs(meta$effect_size/qnorm(meta$pvalue/2)), meta$standard_error)
      meta_est <- suppressWarnings(metafor::rma(yi = meta$effect_size,
                                                sei = meta$standard_error))
    }
    id_list[[i]] <- g[g$variant_id == unique(snp_ind$variant_id)[i], ][1, ]
    if (nrow(meta) > 1) {
      id_list[[i]]$effect_size <- meta_est$beta
      id_list[[i]]$standard_error <- meta_est$se
    }
  }
  g <- do.call("rbind", id_list)
  cat("> In case of multiple effect sizes (there were", multiOR_l,
      "occurrences), used meta-analytic estimate for pooled effect size.\n")
  e$multiple_effectsizes <- multiOR_l
  v <- subset(v, ID %in% unique(g$variant_id))
  g <- subset(g, variant_id %in% unique(v$ID))
  unique_variants <- unique(v$ID)
  radf <- list()
  for (i in 1:length(unique_variants)) {
    RA_g <- g$risk_allele[g$variant_id == unique_variants[i]]
    RA_v <- v$ALT[v$ID == unique_variants[i]]
    radf[[i]] <- c(RA_g, RA_v)
  }
  radf <- data.frame(do.call("rbind", radf))
  colnames(radf) <- c("risk_allele_gwas", "risk_allele_vcf")
  rownames(radf) <- unique_variants

  # strand flip / reverse allele coding
  radf$ref_allele_vcf <- v$REF[v$ID %in% rownames(radf)]
  radf$complement_base_gwas <- rep(NA, nrow(radf))
  radf$complement_base_vcf <- rep(NA, nrow(radf))
  radf$complement_base_vcf_ref <- rep(NA, nrow(radf))
  strandflip <- function(e) switch(e, A = "T", T = "A", C = "G", G = "C")
  for (i in 1:nrow(radf)) {
    s1 <- strandflip(radf$risk_allele_gwas[i])
    s2 <- strandflip(radf$risk_allele_vcf[i])
    s3 <- strandflip(radf$ref_allele_vcf[i])
    radf$complement_base_gwas[i] <- ifelse(is.null(s1), NA, s1)
    radf$complement_base_vcf[i] <- ifelse(is.null(s2), NA, s2)
    radf$complement_base_vcf_ref[i] <- ifelse(is.null(s3), NA, s3)
  }
  radf$different <- radf$risk_allele_gwas != radf$risk_allele_vcf & radf$complement_base_gwas != radf$risk_allele_vcf
  radf$same_as_ref <- radf$risk_allele_gwas == radf$ref_allele_vcf | radf$risk_allele_gwas == radf$complement_base_vcf_ref
  radf$reverse_sign <- ifelse(radf$different == FALSE, 0, 1)
  reverse_sign_SNPs <- rownames(radf)[which(radf$reverse_sign == 1)]
  cat(">", length(reverse_sign_SNPs), "variants had opposite risk allele coding with the GWAS catalog.\n")
  e$opposite_allele_coding <- length(reverse_sign_SNPs)
  g$effect_size_final <- g$effect_size
  reverse_ind <- which(g$variant_id %in% reverse_sign_SNPs)
  g$effect_size_final[reverse_ind] <- -1 * g$effect_size_final[reverse_ind]
  cat(">", length(reverse_ind), "effect sizes had their sign inverted. The new effect sizes are stored as effect_size_final.\n")
  e$inverted_sign <- length(reverse_ind)

  # extreme effect sizes for binary traits
  if (exclude_extreme_associations == TRUE) {
    if (binary_outcome == TRUE) {
      extreme_variants <- unique(g$variant_id[which(abs(log(g$or_per_copy_number)) >
                                                      1.6)])
      g <- subset(g, !variant_id %in% extreme_variants)
      v <- subset(v, !ID %in% extreme_variants)
      if (length(extreme_variants) > 0) {
        cat("> Dropped", length(extreme_variants), "variants with extreme ORs (>5 or <1/5).\n")
      }
      e$extreme_effectsize_variants_dropped <- length(extreme_variants)
      unique_variants <- v$ID
      drop_check(v)
    }
    else {
      cat("> No variants dropped (exclude_extreme_associations only compatible with binary outcomes).\n")
      e$extreme_effectsize_variants_dropped <- 0
    }
  }
  d <- variant_data$gt
  d <- d[, which(colnames(d) %in% unique_variants)]
  LD_dat <- cor(d)
  LD <- LD_dat
  if (LDplot == TRUE) {
    heatmap(LD)
  }

  # pruning variants based on LD
  LD2 <- round(LD^2, 2)
  LDlotri <- lower.tri(LD2) * 1
  LDlotri[LDlotri == 0] <- NA
  LD2 <- LD2 * LDlotri
  LDtab <- which(LD2 >= pruning_threshold, arr.ind = TRUE)
  if (nrow(LDtab) > 0) {
    LDtab2 <- data.frame(matrix(NA, nrow = nrow(LDtab), ncol = 2))
    for (i in 1:nrow(LDtab)) {
      LDtab2[i, 1] <- rownames(LD2)[LDtab[i, 1]]
      LDtab2[i, 2] <- colnames(LD2)[LDtab[i, 2]]
    }
    colnames(LDtab2) <- c("rowSNP", "colSNP")
    LDtab2$rep <- c(0, (diff(as.numeric(factor(LDtab2$rowSNP))) == 0) * 1)
    LDtab2 <- suppressWarnings(reshape(LDtab2, idvar = "rowSNP", timevar = "rep", direction = "wide"))
    keep_SNPs <- rep(NA, nrow(LDtab2))
    for (i in 1:nrow(LDtab2)) {
      vn <- as.character(na.omit(as.character(LDtab2[i, grep("SNP", colnames(LDtab2))])))
      maf <- sapply(d[vn], sum)
      freq <- which(maf == max(maf))[1]
      keep_SNPs[i] <- LDtab2[i, freq]
    }
    remove_SNPs <- setdiff(na.omit(unique(unlist(LDtab2))),
                           keep_SNPs)
    d <- d[, -which(colnames(d) %in% remove_SNPs)]
    g <- subset(g, !variant_id %in% remove_SNPs)
    v <- subset(v, !ID %in% remove_SNPs)
    cat("> Dropped ", length(remove_SNPs), " variants in high LD (R^2>=",
        pruning_threshold, ") with other variants.\n", sep = "")
    e$high_LD_snps_pruned <- length(remove_SNPs)
    drop_check(v)
  }
  if (nrow(LDtab) == 0) {
    e$high_LD_snps_pruned <- 0
  }

  ## make effects conditional
  if(conditional){
    pos <- g$POS
    chr <- g$CHR
    LD2 <- cor(d)

    for(i in 1:nrow(LD2)){
      for(j in 1:ncol(LD2)){
        chr_i <- chr[i]
        chr_j <- chr[j]
        if(chr_i != chr_j){
          LD2[i, j] <- 0
          next
        }
        pos_i <- pos[i]
        pos_j <- pos[j]
        pos_dif <- abs(pos_i - pos_j)
        if(pos_dif > cond_window){
          LD2[i, j] <- 0
        }
        else next
      }
    }

    # convert modified correlation matrix to covariance matrix
    se <- g$standard_error
    sds <- diag(sqrt(diag(cov(d))))
    covX <- sds %*% LD2 %*% sds
    cond_res <- polygenic::marg2con(marginal_coefs = g$effect_size_final,
                                    covX = covX,
                                    N = cond_N,
                                    estimate_se = TRUE,
                                    marginal_se = se)
    g$effect_size_final <- cond_res$beta
    g$standard_error <- cond_res$se
    g$pvalue <- cond_res$p
    cat("> Marginal estimates, standard errors and p-values were converted to conditional based on LD.\n")
  }

  # p-value thresholding
  prem_SNPs <- unique(g$variant_id[which(g$pvalue > pval_threshold)])
  if (length(prem_SNPs) > 0) {
    g <- subset(g, pvalue <= pval_threshold)
    d <- d[, -which(colnames(d) %in% prem_SNPs)]
    v <- subset(v, !ID %in% prem_SNPs)
  }
  cat("> Removed ", length(prem_SNPs), " variants by p-value thresholding (threshold=", pval_threshold, ")\n", sep = "")
  e$SNPs_removed_by_pval_thresholding <- length(prem_SNPs)
  remaining_l <- length(unique(v$ID))
  cat(">", remaining_l, "variants remaining.\n")
  e$remaining_snps <- remaining_l
  drop_check(v)
  g <- g[match(colnames(d), g$variant_id), ]
  effectsize <- as.matrix(g$effect_size_final)

  ## convert to score
  prs <- as.matrix(d) %*% effectsize
  cat("> Weighted polygenic score created using", length(effectsize),
      "SNPs.\n")
  if (scale == TRUE) {
    prs <- scale(prs)
    cat("> Centered and standardized the score.\n")
  }
  e <- do.call("c", e)
  res_list$process_log <- e
  res_list$gwas_info <- g
  res_list$variant_info <- v
  res_list$allele_data <- d
  res_list$risk_allele_df <- radf
  res_list$sample_ids <- as.matrix(rownames(d))
  res_list$prs <- data.frame(id = res_list$sample_ids, prs = prs)
  rownames(res_list$prs) <- NULL
  res_list$ld_matrix <- LD_dat
  if (nrow(LDtab) > 0) {
    res_list$ld_pairs <- LDtab2
  }
  cat("> Returned a list of results with the following dimensions:\n")
  print(lapply(res_list, dim))
  end_time <- Sys.time()
  cat("> Creation of the PRS took", difftime(end_time, start_time,
                                             units = "secs"), "seconds.\n")
  if (flowchart == TRUE) {
    print(PRISMAstatement::flow_exclusions(incl_counts = c(e[1],
                                                           e[1] - e[2], e[1] - e[2] - e[3], e[1] - e[2] - e[3] - e[4],
                                                           e[1] - e[2] - e[3] - e[4] - sum(e[5:6]),
                                                           e[1] - e[2] - e[3] - e[4] - sum(e[5:6]) - e[10],
                                                           e[1] - e[2] - e[3] - e[4] - sum(e[5:6]) - e[10] - e[11],
                                                           e[1] - e[2] - e[3] - e[4] - sum(e[5:6]) - e[10] - e[11] - e[12]),
                                           total_label = "Total variants retrieved",
                                           incl_labels = c("High imputation quality variants",
                                                           "Variants with non-zero variance",
                                                           "Variants reported in GWAS catalog",
                                                           "Variants with complete information",
                                                           "Variants with non-extreme effect sizes",
                                                           "Variants sufficiently independent",
                                                           "Variants used in the weighted score"),
                                           excl_labels = c(paste0("Low imputation quality (R^2<", imp_threshold, ")"),
                                                           "Variants with zero variance in the data",
                                                           "Variants not found in GWAS catalog information",
                                                           "Missing effect size or risk allele",
                                                           "Variants with extreme effect sizes (99th percentile)",
                                                           "Variants in high LD with other included variants with higher MAF",
                                                           paste0("Variants with p-value above threshold (", pval_threshold, ")"))))
  }
  invisible(return(res_list))
}
